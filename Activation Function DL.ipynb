{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae4f701c",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92d0a8f",
   "metadata": {},
   "source": [
    "# They are mathematical functions applied to the output of individual neurons in a neural network. They introduce non-linearty into the network, allowing it to learn and approximate complex relationships between inputs and outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c609f28e",
   "metadata": {},
   "source": [
    " Some commonly used activation function in deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0627e868",
   "metadata": {},
   "source": [
    " 1. Sigmoid Function (logistic function): It maps the input to a value between 0 and 1 . It was widely used in the past but now less popular due to some drawbacks such as vainishing gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0475a6df",
   "metadata": {},
   "source": [
    "2. Hyperbolic tangent function(tanh): Similar to the sigmoid function , but it maos the input to a value between -1 and 1 . It is still used in some cases but it also suffers from vanishing gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8656e7a9",
   "metadata": {},
   "source": [
    " 3. Rectified Linear Unit(Relu): This function sets all negative values to zeros and keeps positive values unchanged . It is the most popular activation function in a deep learning due to its simplicity and effectiveness in training deep neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2ec660",
   "metadata": {},
   "source": [
    " 4. Leaky ReLu: This function is similar to Relu but allows a small negative slope for neagtive input values. It helps mitigate the dying relu problem where some neurons can become permanently inactive during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adb939b",
   "metadata": {},
   "source": [
    " 5. Parametric Relu(PRelu): It is a genearalization of Relu that intoduces a learnable parameter to determine the slope of negative input values . It offers more flexibilty and can imporve model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba2a0a4",
   "metadata": {},
   "source": [
    "6. Exponential Linear Unit(ELU) : It is a varition of Relu that allow negative values with a smooth exponential decay . It helps alleviate the dying Relu problem and can produce more robust models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2a75cf",
   "metadata": {},
   "source": [
    "7. Softmax : It is commonly used in the output layer of a neural network for mutli class classification problems . It normalizes teh output values to represent probalities , ensuring that the sum of all probabilities is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c398b01f",
   "metadata": {},
   "source": [
    "# Activation function are essential in Dl for the following reasons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae84cfb",
   "metadata": {},
   "source": [
    "1. Non- linearity : AF introduce non-linear transformation to the network , enabling it to learn complex patterns and relationships in the data without activation functions , a neural netwrok would simply be a linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fcd99f",
   "metadata": {},
   "source": [
    " 2. Gradient propagation: AF helps propagate gradient backward during the training process , allowing efficient optimiztion and learning .Different activation function have differet characteristics in terms of gradient behvaiuor which can imapact the models training dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9ca47d",
   "metadata": {},
   "source": [
    "3. Model Capacity: The choice of activation function can influence the capacity and expressive power of a neural network . Non - linear activation functions enable the network to represent more complex functions, expanding its ability to learn and generalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af18a1d2",
   "metadata": {},
   "source": [
    " By using suitable activation function , deep learning models can learn and approximate highly non-linear functions, making them powerful tools for tasks such as image recognition , natural language processing , and speech recoginition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0e92ca",
   "metadata": {},
   "source": [
    "# 1.Sigmoid function : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "043f2bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid ( 2.5 )= 0.9241418199787566\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "#ex\n",
    "\n",
    "x=2.5\n",
    "result=sigmoid(x)\n",
    "print(\"Sigmoid (\", x,\")=\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62b9106",
   "metadata": {},
   "source": [
    "# 2. tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a183a29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3fab10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jyoti\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85d505b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genearte a synthetic classification dataset\n",
    "X,y = make_classification (n_samples = 1000, n_features =10 , n_informative=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d7ae0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train , X_test , y_train , y_test = train_test_split(X,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26f73566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jyoti\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the neural network model\n",
    "model=Sequential()\n",
    "model.add(Dense(64,activation='tanh', input_shape=(X_train.shape[1],))) #INPUT layer\n",
    "model.add(Dense(64, activation='tanh'))# hidden layer\n",
    "model.add(Dense(1, activation='sigmoid')) #target layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cf34a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jyoti\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Complie the model\n",
    "model.compile(optimizer='adam' , loss='binary_crossentropy' , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c294cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\jyoti\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jyoti\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "25/25 [==============================] - 1s 1ms/step - loss: 0.5904 - accuracy: 0.6625\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4277 - accuracy: 0.8125\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3981 - accuracy: 0.8200\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3776 - accuracy: 0.8375\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3541 - accuracy: 0.8537\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3397 - accuracy: 0.8525\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3238 - accuracy: 0.8612\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3149 - accuracy: 0.8662\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2963 - accuracy: 0.8737\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2794 - accuracy: 0.8850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1ea5a10e810>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train , y_train , epochs=10 , batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd266011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2478 - accuracy: 0.8950\n",
      "Test Loss: 0.2478376179933548\n",
      "Test Accuracy: 0.8949999809265137\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the testing data\n",
    "loss, accuracy = model.evaluate (X_test , y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8926c6",
   "metadata": {},
   "source": [
    "# 3. ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc02d11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe556399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "input_size=4\n",
    "hidden_size=8\n",
    "output_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8786d732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model=keras.Sequential([\n",
    "    keras.layers.Dense(hidden_size, activation='relu', input_shape=(input_size,)),\n",
    "    keras.layers.Dense(output_size)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4dc6e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complie the model\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
    "             loss=keras.losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96feca10",
   "metadata": {},
   "source": [
    "# Define your input and target data as numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "580dbab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data= np.array([[1.0,2.0,3.0,4.0],\n",
    "                    [2.0,3.0,4.0,5.0],\n",
    "                     [3.0,4.0,5.0,6.0]])\n",
    "target_data=np.array([[5.0,0.8],\n",
    "                     [0.6,0.9],\n",
    "                     [0.7,1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a72500df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1ea5a3e0c50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(input_data, target_data, epochs=1000, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13b24000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 65ms/step\n"
     ]
    }
   ],
   "source": [
    "# Test the mdoel\n",
    "test_input= np.array([[1.0,2.0,3.0,4.0]])\n",
    "predicted_output=model.predict(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0a79bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output: [[4.21185   0.7975764]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Predicted Output: {predicted_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11046ba",
   "metadata": {},
   "source": [
    "# 4. Leaky ReLu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ff8eeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.2  2.  -0.1  3. ]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the input tensor\n",
    "input_tensor = tf.constant([-1.0,2.0,-0.5,3.0])\n",
    "\n",
    "# Apply leaky ReLu activation function\n",
    "output_tensor = tf.nn.leaky_relu(input_tensor , alpha=0.2)\n",
    "\n",
    "# print the output\n",
    "print(output_tensor.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb094b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91ab4ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "input_size=4\n",
    "hidden_size=8\n",
    "output_size=2\n",
    "\n",
    "# Create the input and target tensors\n",
    "inputs= tf.keras.Input(shape=(input_size,))\n",
    "targets = tf.keras.Input(shape=(output_size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42de266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the weights and biases for the hidden layer\n",
    "hidden_weights = tf.Variable(tf.random.normal(shape=(input_size,hidden_size)))\n",
    "hidden_biases=tf.Variable(tf.zeros(shape=(hidden_size,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6898c1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(4, 8) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(8,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"
     ]
    }
   ],
   "source": [
    "# Compute the hidden layer output with Leaky Relu activation function\n",
    "hidden_layer_output = tf.nn.leaky_relu(tf.matmul(inputs, hidden_weights)+hidden_biases, alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df048be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the weights and biases for the output layer\n",
    "output_weights = tf.Variable(tf.random.normal(shape=(hidden_size,output_size)))\n",
    "output_biases= tf.Variable(tf.zeros(shape=(output_size,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12e4a9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_1), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(8, 2) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_1), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(2,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"
     ]
    }
   ],
   "source": [
    "# compute the final output\n",
    "output=tf.matmul(hidden_layer_output,output_weights)+output_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc76be79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function\n",
    "\n",
    "loss=tf.reduce_mean(tf.square(output-targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9281fe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizer\n",
    "optimizer=tf.keras.optimizers.SGD(learning_rate=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a79770ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model=tf.keras.Model(inputs=[inputs, targets], outputs=output)\n",
    "model.add_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10768482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5521b87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your input and target data as numpy arrays\n",
    "input_data =np.array([[1.0,2.0,3.0,4.0]])  #Replace with your input data\n",
    "target_data = np.array([[0.5,0.8]]) #Replace the traget data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1e49a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1ea5b420050>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit([input_data, target_data], epochs=1000, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62faa3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    }
   ],
   "source": [
    "# Test the trained network\n",
    "test_input=np.array([[1.0,2.0,3.0,4.0]])\n",
    "test_target=np.array([[0.0,0.0]]) #Dumpy traget for prediction , not used\n",
    "predicted_output= model.predict([test_input, test_target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "38bae543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output:[[ 10.10327  -12.118675]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Predicted Output:{predicted_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b178c0b7",
   "metadata": {},
   "source": [
    "# 5. Parametric Rectified linear unit (PReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cc42b9",
   "metadata": {},
   "source": [
    "# The parametric Rectified Linear Unit(PReLU) activation function is an extension of the leaky Relu activation function that allows the slope of the negative part of the function to be learned during the training process.Instead of using afixed slope value , PRelu intoduce a set of learnable parameters tha control the slope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6157274",
   "metadata": {},
   "source": [
    "The PReLU function is defned as foolows!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795b3a53",
   "metadata": {},
   "source": [
    "PReLU(x)=max(0,x)+alpha*min(0,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d62774",
   "metadata": {},
   "source": [
    "where x is the input value and alpha is a learnable parametre vector of the same shape as x . The aplha parameter determine the slope of negative inputs , allowing it to be different for each neurons or channel in a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce225f75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
